---
layout: post
title: Assignment - Blog Post 3
---

## Introduction

In this post, we'll be making a fake news classifier that'll be able to predict whether or not a given news article is fake or not. We'll be working with Google's TensorFlow package to generate an advanced machine learning model using neural networks. In particular we'll be using TensorFlow's functional API to manually make all of our layers. 

For more information on neural networks feel free to visit this helpful <a href="https://en.wikipedia.org/wiki/Artificial_neural_network"> Wikipedia post</a>. For more information on TensorFlow, feel free to visit the <a href="https://www.tensorflow.org/">TensorFlow documentation</a>. 

> Data Source

The data we'll be using comes from the article: 

- Ahmed H, Traore I, Saad S. (2017) “Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques. In: Traore I., Woungang I., Awad A. (eds) Intelligent, Secure, and Dependable Systems in Distributed and Cloud Environments. ISDDC 2017. Lecture Notes in Computer Science, vol 10618. Springer, Cham (pp. 127-138).

It was accesed from <a href="https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset">Kaggle</a>, and has already gone through a fair amount of data processing thanks to <a href="https://www.philchodrow.com/"> Phil Chodrow</a>.

In this post we'll construct three different models one just using the article's title, another using only the text, and finally one using both the tile and the text. We'll choose the best of these models to evaluate on unseen data. 

## Data Processing

> Imports

First, as always we must import the necessary packages that we'll be using throughout this post.

```python
# General Imports
import numpy as np
import pandas as pd

# We'll use these as part of our make_dataset() function
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# We'll use thise as part of our standardization() function
import re
import string

# We'll use these for our models
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# We'll use these for our embedding vizualization
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"
```

> Loading Data

Let's load and take a look at the raw data that we'll be using.

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
df
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>10709</td>
      <td>ALARMING: NSA Refuses to Release Clinton-Lynch...</td>
      <td>If Clinton and Lynch just talked about grandki...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>8731</td>
      <td>Can Pence's vow not to sling mud survive a Tru...</td>
      <td>() - In 1990, during a close and bitter congre...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>4733</td>
      <td>Watch Trump Campaign Try To Spin Their Way Ou...</td>
      <td>A new ad by the Hillary Clinton SuperPac Prior...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>3993</td>
      <td>Trump celebrates first 100 days as president, ...</td>
      <td>HARRISBURG, Pa.U.S. President Donald Trump hit...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>12896</td>
      <td>TRUMP SUPPORTERS REACT TO DEBATE: “Clinton New...</td>
      <td>MELBOURNE, FL is a town with a population of 7...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>

> TensorFlow DataSet

Next, we'll need to define a function that'll do two things:

1. We need to remove the <i>stopwords</i> from the "title" and "text" columns from our dataset. These are words like "the", "and", "but" etc. that don't add much value when training our model. 
2. We need to construct and return a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">tf.data.Dataset</a> with two inputs and one output. A tf.data.Dataset is a subset of the TensorFlow Dataset class. The use of these datasets can make it significantly easier to stay organized when organizing data structures. 

```python
def make_dataset(df):

  # Removing Stop words:

  stop = stopwords.words("english")

  for i in ["text", "title"]:
    df[i] = df[i].str.lower().str.split()
    df[i] = df[i].apply(lambda x: [item for item in x if item not in stop])
    df[i] = df[i].apply(" ".join)

  # Making tensorflow dataset
  data = tf.data.Dataset.from_tensor_slices(
      (
        {
          "title" :  df[["title"]],
          "text" : df[["text"]]
       },

        {
          "fake" : df[["fake"]]
        }
      )
  )

  return data
```

Now, let's call this function on our raw data and take a look at the first five inputs of our tf.data.Dataset.

```python
data = make_dataset(df)

for input, output in data.take(5):
    print(input)
    print(output)
    print("")
```

	{'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b"merkel: strong result austria's fpo 'big challenge' parties"],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'german chancellor angela merkel said monday strong showing austria anti-immigrant freedom party (fpo) sunday election big challenge parties. speaking news conference berlin, merkel added hoping close cooperation austria conservative election winner sebastian kurz european level.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'trump says pence lead voter fraud panel'], dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'west palm beach, fla.president donald trump said remarks broadcast sunday would put vice president mike pence charge commission probe believes voter fraud last november\'s election. overwhelming consensus among state officials, election experts, politicians voter fraud rare united states, trump repeatedly said thinks perhaps millions votes cast nov. 8 election fraudulent. "i\'m going set commission headed vice president pence we\'re going look very, carefully," trump told fox news channel\'s bill o\'reilly interview taped friday. trump, spending weekend mar-a-lago resort palm beach, florida, captured presidency winning enough state-by-state electoral college votes defeat democrat hillary clinton. still, clinton popular vote nearly 3 million votes, piling overwhelming majority deeply democratic states like california. irked trump result claimed voter fraud without evidence. senate majority leader mitch mcconnell, kentucky republican, said \'s "state union" election fraud occur "there evidence occurred significant number would changed presidential election." "and think ought spend federal money investigating that. think states take look issue," said.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'in: suspected leaker \xe2\x80\x9cclose confidant\xe2\x80\x9d james comey reassigned post top fbi lawyer [video]'],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'december 5, 2017, circa sara carter warned would major shake-up fbi inspector general report completed. far, sara carter right everything reported on, relates mueller investigation. below, carter tells sean hannity believes fbi major shake-up soon 27 leakers ig looking at! yes, 27 leakers!sara carter: going see parts report december (end month). going see parts report coming january. looking peter strzok. looking comey. looking 27 leakers. would surprise shake-up fbi housecleaning.watch:is fbi former top attorney, james baker, one first leaker casualties? james baker, fbi leading lawyer confidante fired fbi director james comey, reassigned post, agency top personnel high scrutiny.baker told colleagues assume different duties bureau, washington post reported.baker oversees bureau office general counsel received awards george h.w. bush award excellence counter-terrorism 2006.he also subject leak investigation summer attorney general jeff sessions ordered crackdown leakers.the fbi comment asked baker reassigned would doing.his reassignment comes time increased scrutiny pressure agency, following release private text messages agents working hillary clinton email probe. daily mail three sources, knowledge investigation, told circa baker top suspect ongoing leak investigation, circa able confirm details national security information material allegedly leaked.a federal law enforcement official knowledge ongoing internal investigations bureau told circa, bureau scouring leakers lot investigations. revelation comes trump administration ramped efforts contain leaks within white house within national security apparatus.baker close confidant former fbi director james comey, recent media reports suggested reportedly advising then-fbi director legal matters following private meetings former director february president trump oval office.baker appointed fbi general counsel comey 2014 long distinguished history within intelligence community.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'thyssenkrupp offered help argentina disappeared submarine'],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'germany thyssenkrupp, offered assistance argentinian authorities investigation disappearance submarine last month, spokesman said friday. ara san juan delivered 1985 built unit thyssen ag, merged krupp form thyssenkrupp 1999. offered support technical investigation tragedy contact argentinian navy respect, spokesman said. said maintenance submarine conducted thyssenkrupp. submarine went missing nov. 15 44 crew members aboard south atlantic waters. navy said nov. 27 water entered submarine snorkel caused battery short circuit went missing. tragedy underscored critics described parlous state argentina military, faced dwindling funding years. german magazine wirtschaftswoche earlier reported delegation argentinian navy traveled kiel northern germany discuss questions submarine thyssenkrupp. identify sources. argentinian president mauricio macri called serious deep investigation incident.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b"trump say appeals court decision travel ban 'political'"],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'president donald trump thursday called appellate court ruling upheld suspension order restricting travel seven muslim-majority countries "political decision," vowed administration would ultimately prevail. "we\'ll see court," trump told reporters gathered outside press secretary\'s office. "it\'s political decision." trump said view ruling major setback white house. "this decision came down, we\'re going win case," said.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>}

> Data Prep

Now we need to prep our data to get it ready for our models. 

First, let's shuffle our data and make training and validation sets. We'll be using 80% of the data for training. 

```python
data = data.shuffle(buffer_size = len(data))

train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size).batch(20)
val   = data.skip(train_size).take(val_size).batch(20)

len(train), len(val)
```

	(898, 225)

> Standardization and Vectorization

Next let's make a standardization() function that we'll use for a standardization layer in our model. This will convert all the characters in the title and text column to lower case and remove punctuation. 

```python
def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation
```

Next, we need to vectorize our data. <i>Vectorization</i> refers to the process of representing text as vectors. There are many different ways to vectorize text, in this case we'll replace each word by it's <i>frequency</i> rank, its ranking in terms of how frequently it appears in the dataset compared to other words.

```python
vocab_size = 2000

vectorize_layer = TextVectorization(
    standardize            = standardization,
    max_tokens             = vocab_size,
    output_mode            = "int",
    output_sequence_length = 500)

vectorize_layer.adapt(train.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
``` 

## Modeling 

Now that we've prepared our data we can start building and training our mdoels. 

> Specify Inputs

The first step is to specify the inputs. Here we're only going to have two inputs, referring to the "title" and "text" coulms from our news article data. 

```python
title_input = keras.Input(
    shape = (1,),
    name = "title",
    dtype = "string"
)

text_input = keras.Input(
    shape = (1,),
    name = "text",
    dtype = "string"
)
```

> Emedding Layer 

For all three of our models we'll be using an <i>embedding</i> layer. Embedding refers to a representation of a word in a vector space. Each word is assigned a vector, the idea is that words that have similar meanings or roots are closer to one another in the vector space, while words that are less related are farther apart. 

Since we'll be using the same type of emebedding layer accross all three of our models, it's best to create a general shared emebedding layer that we can simply call for each of our models. 

```python
shared_embedding = layers.Embedding(vocab_size, 10, name = "embedding")
```

Now we have everything we need to start modeling. 

> Model 1 (Title Only)

First, we need to specify the input layers we want to use for our model. We'll be using the same set up for all three of our models. 

```python
title_features = vectorize_layer(title_input)
title_features = shared_embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
```
Next we need to specify out output layer.

```python
output_title = layers.Dense(2, name = "fake")(title_features)
```

Now, lets make our model and take a look at the summary.

```python
model_title = keras.Model(
    inputs = title_input,
    outputs = output_title
)

model_title.summary()
```

	Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 500)               0         
    _________________________________________________________________
    embedding (Embedding)        (None, 500, 10)           20000     
    _________________________________________________________________
    dropout (Dropout)            (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d (Gl (None, 10)                0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense (Dense)                (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________

Let's take a look at what this model looks like visually.

```python
keras.utils.plot_model(model_title)
```

![model_title_modelplot.png](/images/model_title_modelplot.png)

Next, let's compile our model.

```python
model_title.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```

Alright now we're ready to train our model! 

```python
history_title = model_title.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```

Let's take a look at how our model is performing on our training data. 

```python
from matplotlib import pyplot as plt
plt.plot(history_title.history["accuracy"], label = "Training")
plt.plot(history_title.history["val_accuracy"], label = "Validations")
plt.legend()
```

![model_title_performance.png](/images/model_title_performance.png)

WOW! Looks like our first model is reaching a top accuracy just under 95% on on training data. That's great for a first attempt, but let's see if we can do even better! 

> Model 2 (Text Only)

Let's make our second model, we'll follow the identical procedure as we did for our first model. 

Specify input layers: 

```python
text_features = vectorize_layer(text_input)
text_features = shared_embedding(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
```

Specify output layer:

```python
output_text = layers.Dense(2, name = "fake")(text_features)
```

Build model:

```python
model_text = keras.Model(
    inputs = text_input,
    outputs = output_text
)

model_text.summary()
```

	Model: "model_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    text (InputLayer)            [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 500)               0         
    _________________________________________________________________
    embedding (Embedding)        (None, 500, 10)           20000     
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d_1 ( (None, 10)                0         
    _________________________________________________________________
    dropout_3 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________

Our second and third models will have very similar representation's as our first one, so we can skip that step.

Compile model:

```python
model_text.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```
Train model:

```python
history_text = model_text.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```

Let's take a look at our our second model performed, did it do better than out first? 

```python
plt.plot(history_text.history["accuracy"], label = "Training")
plt.plot(history_text.history["val_accuracy"], label = "Validations")
plt.legend()
```

![model_text_performance.png](/images/model_text_performance.png)

AMAZING! Looks like our model is reaching just above 97.5% accuracy on our training data. So this tells us that it's easier for a computer to determine if a news article is fake or not based on the actual content of the article as opposed to just the title. 

Let's take a look at our last model, we should expect that it performs even better than both of our previous ones, just think about when you look at an article, if you read both the title and the actual content you'll have an easier time determingg if it's fake or not than just by looking at one of the two right? 

> Model 3 (Title and Text)

Since we're using the same two inputs from our previous two models, all we need to do is concatenate the input layers we used earlier. This is where the shared embedding layer comes in handy. If we had simply specified individual embedding layers for our previous models then we couldn't concatenate them as there would be two embeddings. 

```python
main_title_text = layers.concatenate([title_features, text_features], axis = 1)
```

Next, let's pass our inputs through one last dense layer and then specify our output layer. 

```python
main_title_text = layers.Dense(32, activation='relu')(main_title_text)
output_title_text = layers.Dense(2, name = "fake")(main_title_text)
```

Great! Now let's make our last model.

```python
model_title_text = keras.Model(
    inputs = [title_input, text_input],
    outputs = output_title_text
)

model_title_text.summary()
```

	Model: "model_2"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    title (InputLayer)              [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text (InputLayer)               [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text_vectorization (TextVectori (None, 500)          0           title[0][0]                      
                                                                     text[0][0]                       
    __________________________________________________________________________________________________
    embedding (Embedding)           (None, 500, 10)      20000       text_vectorization[0][0]         
                                                                     text_vectorization[1][0]         
    __________________________________________________________________________________________________
    dropout (Dropout)               (None, 500, 10)      0           embedding[0][0]                  
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 500, 10)      0           embedding[1][0]                  
    __________________________________________________________________________________________________
    global_average_pooling1d (Globa (None, 10)           0           dropout[0][0]                    
    __________________________________________________________________________________________________
    global_average_pooling1d_1 (Glo (None, 10)           0           dropout_2[0][0]                  
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 10)           0           global_average_pooling1d[0][0]   
    __________________________________________________________________________________________________
    dropout_3 (Dropout)             (None, 10)           0           global_average_pooling1d_1[0][0] 
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 32)           352         dropout_1[0][0]                  
    __________________________________________________________________________________________________
    dense_1 (Dense)                 (None, 32)           352         dropout_3[0][0]                  
    __________________________________________________________________________________________________
    concatenate (Concatenate)       (None, 64)           0           dense[0][0]                      
                                                                     dense_1[0][0]                    
    __________________________________________________________________________________________________
    dense_2 (Dense)                 (None, 32)           2080        concatenate[0][0]                
    __________________________________________________________________________________________________
    fake (Dense)                    (None, 2)            66          dense_2[0][0]                    
    ==================================================================================================
    Total params: 22,850
    Trainable params: 22,850
    Non-trainable params: 0
    __________________________________________________________________________________________________


Now, let's compile and train our model! 

```python
model_title_text.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```
```python
history_title_text = model_title_text.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)
```

Let's take a look at how or last model did.

```python
plt.plot(history_title_text.history["accuracy"], label = "Training")
plt.plot(history_title_text.history["val_accuracy"], label = "Validations")
plt.legend()
```

![model_title_text_performance.png](/images/model_title_text_performance.png)

INCREDIBLE! As expected our last model is performing way better on the training data than our previous two! It's reaching more than 99.9% accuracy on the validation data. 

We'll ue this last model to verify against unseen data. 

## Model Evaluation

First, let's read in and look at our test data.

```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_df = pd.read_csv(test_url)
test_df
```

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>420</td>
      <td>CNN And MSNBC Destroy Trump, Black Out His Fa...</td>
      <td>Donald Trump practically does something to cri...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14902</td>
      <td>Exclusive: Kremlin tells companies to deliver ...</td>
      <td>The Kremlin wants good news.  The Russian lead...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>322</td>
      <td>Golden State Warriors Coach Just WRECKED Trum...</td>
      <td>On Saturday, the man we re forced to call  Pre...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16108</td>
      <td>Putin opens monument to Stalin's victims, diss...</td>
      <td>President Vladimir Putin inaugurated a monumen...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>10304</td>
      <td>BREAKING: DNC HACKER FIRED For Bank Fraud…Blam...</td>
      <td>Apparently breaking the law and scamming the g...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>22444</th>
      <td>20058</td>
      <td>U.S. will stand be steadfast ally to Britain a...</td>
      <td>The United States will stand by Britain as it ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22445</th>
      <td>21104</td>
      <td>Trump rebukes South Korea after North Korean b...</td>
      <td>U.S. President Donald Trump admonished South K...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22446</th>
      <td>2842</td>
      <td>New rule requires U.S. banks to allow consumer...</td>
      <td>U.S. banks and credit card companies could be ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>22447</th>
      <td>22298</td>
      <td>US Middle Class Still Suffering from Rockefell...</td>
      <td>Dick Eastman The Truth HoundWhen Henry Kissin...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>22448</th>
      <td>333</td>
      <td>Scaramucci TV Appearance Goes Off The Rails A...</td>
      <td>The most infamous characters from Donald Trump...</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>22449 rows × 4 columns</p>
</div>

Let's prepare the test dataset by calling our make_dataset() function with it.

```python
test_data = make_dataset(test_df)

for input, output in test_data.take(5):
    print(input)
    print(output)
    print("")
```

	{'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'cnn msnbc destroy trump, black fact-free tax reform speech (tweet)'],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'donald trump practically something criticize media fake news almost daily basis, two major news networks turned tables put blast major way.the white house currently arms recent decision msnbc, decided air trump insanely dishonest north dakota speech earlier today. trump supposed address tax reform speech, major problem saying word true. response fact-free speech, msnbc decided irresponsible air speech decided spare americans misinformation nonsense.once trump administration found move press, white house practically threw hissy fit. special assistant president assistant white house communications director steven chueng tweeted:instead insulting viewers trump lies, msnbc nicole wallace reported important, truthful topics daca russia. jake tapper focused hurricane irma, russia, debt ceiling trump spewed usual nonsense.this major f*ck trump administration sends strong message dishonest narrative administration continues put american people tolerated. people likely going react strongly topic dry tax reform, two major networks refusing air speech big deal. trump desperately wants needs coverage, obviously pissed eyes him. cable news networks break trump, already failing presidency even danger.featured screenshot'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'exclusive: kremlin tells companies deliver good news'],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'kremlin wants good news. russian leadership told major companies supply news stories put stewardship country positive light, according documents seen . seven-page document spelled kind articles required, focus new jobs, scientific achievements new infrastructure, especially involving state support. also detailed stories presented, gave weekly deadline submissions. instructions sent last month energy ministry 45 companies russia energy utilities sector including rosneft, lukoil novatek, according second document, list recipients. drive coincides run-up presidential election march next year president vladimir putin needs strong mandate high turnout maintain firm grip power dominating russian politics two decades. life majority people become calmer, comfortable, attractive. many examples often escape media attention, said first document. task, creative painstaking approach, select topics subjects offer media. document, mention election, said news items supplied feed positive news correspond two themes: life getting better things were; . documents attached invitation, dated oct. 9, sent energy ministry senior executives public relations government relations departments firms, 17 state-controlled 28 privately-held. invitation requested send representatives oct. 12 meeting ministry moscow discuss help government pr effort. saw copy invitation spoke three executives received it. according invitation, news initiative requested sergei kiriyenko, first deputy chief staff presidential administration. spokesman kiriyenko respond request comment. energy ministry also respond, kremlin spokesman dmitry peskov. sent requests comment biggest five companies 45, market value - state-owned oil major rosneft, state-owned gas giant gazprom, private oil companies lukoil surgutneftegaz, private gas firm novatek. responses received. nine-point list oil gas provide russia biggest source revenue energy firms among powerful companies biggest employers. found evidence similar instructions sent companies sectors. oct. 12 meeting chaired deputy energy minister anton inyutsyn, official presidential administration also present, according one sources attended. two officials went explained instructions laid seven-page document, said source, added election mentioned. reported february ministry enlisted energy companies give advance notice developments could influence public opinion. meeting last month guidelines circulated preparation show that, since then, initiative stepped higher gear, companies handed highly specific instructions expected help. clear companies acted instructions. news guidelines document said government wanted highlight victories achievements . included nine-point list kind news companies supply. asked, example, stories business units possible say state support helped lift crisis, restored modern production, re-equipped new equipment gave work local residents . examples given kind events interest government elsewhere corporate world included state lender sberbank hiring 700 people volga river city togliatti, festival funded company kaliningrad region young people hearing difficulties sports center opened cherkessk, southern russia. document also held case yevgeny kosmin example positive news story, miner western siberia whose team extracted 1.6 million tonnes coal july year, monthly record. carried echoes alexey stakhanov, miner 1935 extracted almost 15 times coal shift quota required. communist propaganda held stakhanov symbol soviet industrial prowess. instructions stipulated companies submit positive news stories every week - monday, tuesday morning latest. said companies present items format table, new additions highlighted colored font, accompanied press release could passed journalists minimal editing government officials. document also required company provide contact person could provide extra information journalists, tell tv news crews reach venue report event, organize access news crews company sites. able establish kremlin made similarly specific demands companies past. putin yet declared intention seek re-election. kremlin observers say will. opinion polls show win comfortably, many voters crediting restoring national pride. kremlin biggest headache election, scheduled next march, ensuring strong turnout, say many political analysts. economy weak many people viewing result foregone conclusion, voters may tempted stay away polling stations. low turnout could undermine putin legitimacy next term, analysts say.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'golden state warriors coach wrecked trump attack one players'],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b"saturday, man forced call president trump responded golden state warriors star stephen curry refusal accept invitation white house internet equivalent incoherently screeching well fine, invited anyway. going white house considered great honor championship team.stephen curry hesitating,therefore invitation withdrawn! donald j. trump (@realdonaldtrump) september 23, 2017unfortunately trump, warriors coach steve kerr curry back. idea civil discourse guy tweeting demeaning people saying things saying sort far-fetched, kerr said sunday. picture us really civil discourse him? actual chance talk president, said. all, works us. public servant. may aware that, public servant, right? maybe nba champions, people prominent position, could go say, bothering us, this?' laid trump attacking black football players exercising first amendment rights calling nazis fine people: irony free speech fine neo-nazi chanting hate slogans, free speech allowed kneel protest? matter many times football player says, honor military, protesting police brutality racial inequality, matter. nationalists saying, disrespecting flag. well, know else disrespectful flag? racism. one way worse other. trump constant embarrassment country.watch kerr rip apart below:featured screen capture"],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b"putin opens monument stalin's victims, dissidents cry foul"],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'president vladimir putin inaugurated monument victims stalinist purges monday, soviet-era dissidents accused cynicism time say authorities riding roughshod civil freedoms. wall grief occupies space edge moscow busy 10-lane ring road depicts mass faceless victims, many sent prison camps executed josef stalin watch falsely accused enemies people. nearly 700,000 people executed great terror 1937-38, according conservative official estimates. unequivocal clear assessment repression help prevent repeated, putin said opening ceremony. terrible past must erased national memory cannot justified anything. words ceremony amounted one strongest condemnations soviet union dark side 18 years dominated russia political landscape. putin past called stalin complex figure said attempts demonise ploy attack russia. monday ceremony, said lessons russia. mean demanding accounts settled, said putin, stressed need stability. must never push society dangerous precipice division. putin carefully balanced words reflect kremlin unease year centenary 1917 bolshevik revolution, paved way stalin rise. uncomfortable promoting discussion idea governments overthrown force, kremlin organizing commemorative events. putin, expected run win presidency march, told human rights activists earlier monday hoped centenary would allow society draw line tumultuous events 1917 accept russia history - great victories tragic pages . yet historians fret say putin ambiguity stalin along russia 2014 annexation ukraine crimea emboldened stalin admirers. monuments memorial plaques honoring stalin sprung different russian regions. state-approved textbooks softened , opinion poll june crowned country outstanding historical figure. contrast, helped document stalin crimes, memorial human rights group individual historians journalists, sometimes felt pressure authorities. group soviet-era dissidents published letter monday, accusing putin cynicism. ... consider opening moscow monument victims political repression untimely cynical, said letter, published kasparov.ru news portal. impossible take part memorial events organized authorities say sorry victims soviet regime, practice continue political repression crush civil freedoms.'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>}
    
    {'title': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'breaking: dnc hacker fired bank fraud\xe2\x80\xa6blames islamophobia \xe2\x80\x9cultra right-wing media\xe2\x80\x9d'],
          dtype=object)>, 'text': <tf.Tensor: shape=(1,), dtype=string, numpy=
    array([b'apparently breaking law scamming government excusable conveniently blame discrimination. media ignoring illegal activity, stopping getting away anything? detained bank fraud charges monday, imran awan attorney recently released statement blaming islamophobia ultra right-wing media :this pakistani family criminal investigation u.s. capitol police abusing access house representatives information technology (it) system. abid, imran jamal awan accessed congressmen congress people computer networks unauthorized engaged myriad questionable schemes besides allegedly placing ghost employees congressional payroll.capitol police revoked awans access congressional system february 2017 major data breach detected. access allowed read emails files dozens members, including many serving house permanent select committee intelligence house committee foreign affairs.imran awan, wife hina, brothers abid jamal collectively netted $4 million salary administrators house democrats 2009 2017. yet absence signs wealth displayed among raise questions. money sent overseas something paychecks motivate actions? imran wife traveling pakistan earlier week carrying $12,000. however, imran arrested could board plane.today, awan attorney released statement claiming attacks mr. awan family began part frenzy anti-muslim bigotry literal heart democracy, house representatives. goes saying utterly unsupported, outlandish, slanderous statements targeting mr. awan coming ultra-right-wing pizzagate media sitting members congress. attorney claims couple traveling see family first time months abruptly unjustly fired. read more: gateway pundit'],
          dtype=object)>}
    {'fake': <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>}

Alright! Let's evaluate our model, fingers crossed! 

```python
model_title_text.evaluate(test_data)
```

	 22449/22449 [==============================] - 39s 2ms/step - loss: 0.1739 - accuracy: 0.9815
    [0.1739414930343628, 0.9814690947532654]


WOW! WOW! WOW! Our model reached 98.15% accuracy on unseen data! 

## Embedding Visualization

Let's take a look at what words are associated with either real or fake news articles. 

```python
weights = model_title_text.get_layer('embedding').get_weights()[0] # get the weights from the emebdding layer
vocab = vectorize_layer.get_vocabulary()    					   # get the vocabulary from our data prep
```

Now let's create an embedding data frame. 

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

embedding_df
```

Alright, we're ready to look at the word associations our model made. 

```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```
{% include embedding_viz.html %}

Though it may be difficult to distinguish which side is fake and which is real, it appears that the words to the left are more associated with fake news and the ones to the right are associated with real news. 

On the fake side we can see posessive pronouns like "We" and "I" as well as reference to "Trump". On the real news side we can see references to political parties like "GOP" as well as words like "apparently" and "reportedly" to add a reasonable level of uncertainty. 