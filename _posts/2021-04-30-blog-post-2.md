---
layout: post
title: Assignment - Blog Post 2
---

## Introduction

In this problem, we'll study *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. 

### Notation

In all the math below: 

- Boldface capital letters like $$\mathbf{A}$$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $$\mathbf{v}$$ refer to vectors (1d arrays of numbers). 
- $$\mathbf{A}\mathbf{B}$$ refers to a matrix-matrix product (`A@B`). $$\mathbf{A}\mathbf{v}$$ refers to a matrix-vector product (`A@v`).


To start, let's look at an example where we *don't* need spectral clustering. 

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

    
![HW2_phil_1.png](/images/HW2_phil_1.png)
    


*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![HW2_phil_2.png](/images/HW2_phil_2.png)
    


### Harder Clustering

That was all well and good, but what if our data is "shaped weird"? 


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```


    
![HW2_phil_3.png](/images/HW2_phil_3.png)
    


We can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![HW2_phil_1.png](/images/HW2_phil_4.png)
    


Whoops! That's not right! 

As we'll see, spectral clustering is able to correctly cluster the two crescents. In the following problems, you will derive and implement spectral clustering. 

## Part A

First, let's construct the *similarity matrix* $$ \mathbf{A}$$.


For tnow, we'll use `epsilon = 0.4`. 


```python
from sklearn.metrics.pairwise import euclidean_distances

epsilon = 0.4

PD = euclidean_distances(X,X)
A = np.ndarray(shape = (n,n))

A[PD < epsilon**2] = 1
A[PD >= epsilon**2] = 0
np.fill_diagonal(A,0)
A
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           ...,
           [0., 0., 0., ..., 0., 0., 1.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 1., 0., 0.]])



## Part B

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. 

Let $$d_i = \sum_{j = 1}^n a_{ij}$$ be the $$i$$th row-sum of $$\mathbf{A}$$, which is also called the *degree* of $$i$$. Let $$C_0$$ and $$C_1$$ be two clusters of the data points. We assume that every data point is in either $$C_0$$ or $$C_1$$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $$i$$ of $$\mathbf{A}$$) is an element of cluster $$C_1$$.  

The *binary norm cut objective* of a matrix $$\mathbf{A}$$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $i$ (the total number of all other rows related to row $$i$$ through $$A$$). The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

A pair of clusters $$C_0$$ and $$C_1$$ is considered to be a "good" partition of the data when $$N_{\mathbf{A}}(C_0, C_1)$$ is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

First, the cut term $$\mathbf{cut}(C_0, C_1)$$ is the number of nonzero entries in $$\mathbf{A}$$ that relate points in cluster $$C_0$$ to points in cluster $$C_1$$. Saying that this term should be small is the same as saying that points in $$C_0$$ shouldn't usually be very close to points in $$C_1$$. 

Let's write a function called `cut(A,y)` to compute the cut term. 



```python
def cut(A,y):
    
    # Create two lists to which we'll append
    # values accordingly
    C0_list = []
    C1_list = []
    
    # Iterate accross all the the values of y
    for i in range(y.shape[0]):
        # Check the value of y at index i
        # Append the index to either C1_list or
        # C0_list depending on the value of y at that index
        if y[i] == 1:
            C1_list.append(i)
        else:
            C0_list.append(i)
    
    # Transform the lists into numpy arrays
    C0 = np.array(C0_list)
    C1 = np.array(C1_list)
    
    # Sum all the different combinations for entries
    # A[i,j] for i and j in each cluster
    cut = sum(sum(A[np.ix_(C0,C1)]))
    
    return cut
```

Let's compute the cut objective for the true clusters `y`. Then, we'll generate a random vector of random labels of length `n`, with each label equal to either 0 or 1, and check the cut objective for the random labels. 


```python
cut(A,y)
```




    0.0




```python
y_rand = np.random.randint(0,2,size=n)
cut(A,y_rand)
```




    373.0

The cut objective for the true labels is *much* smaller than the cut objective for the random labels!

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 


#### B.2 The Volume Term 

Now, let's take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is. If we choose cluster $$C_0$$ to be small, then $$\mathbf{vol}(C_0)$$ will be small and $$\frac{1}{\mathbf{vol}(C_0)}$$ will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters $$C_0$$ and $$C_1$$ such that:

1. There are relatively few entries of $$\mathbf{A}$$ that join $$C_0$$ and $$C_1$$. 
2. Neither $$C_0$$ and $$C_1$$ are too small. 

We'll first write a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple. 

Then, we'll write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 


```python
def vols(A,y):
    
    # Determine C0 and C1 based on values of y
    C0 = np.where(y == 0)[0]
    C1 = np.where(y == 1)[0]
    
    # Compute the values of vol0 and vol1 by summing
    # the values of A indexed at each cluster
    vol0 = sum(sum(A[C0]))
    vol1 = sum(sum(A[C1]))
    
    return vol0, vol1

def normcut(A,y):
    
    vol0, vol1 = vols(A,y)
    
    return cut(A,y)*(1/vol0 + 1/vol1)
```

Now, let's compare the `normcut` objective using both the true labels `y` and the fake labels you generated above.


```python
print(normcut(A,y))
print(normcut(A,y_rand))
```

    0.0
    0.9877184588483989

 As we can see above, the normcut for the true labels is *much* smaller than the normcut of our fake labels.


## Part C

We have now defined a normalized cut objective which takes small values when the input clusters are: 

1. Joined by relatively few entries in $$A$$ 
2. Not too small. 

One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 


1. First, we'll write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given `A` and `y`, using the formula above. 
2. Then, we'll check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 
3. While we're here, let's also check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries. 

#### Programming Note

We can compute $$\mathbf{z}^T\mathbf{D}\mathbf{z}$$ as `z@D@z`, provided that you have constructed these objects correctly. 

#### Note

The equation above is exact, but computer arithmetic is not! `np.isclose(a,b)` is a good way to check if `a` is "close" to `b`, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify. 


```python
def transform(A,y):
    
    vol0, vol1 = vols(A,y)
    
    z = np.zeros(y.shape[0])
    
    z[y == 0] = 1/vol0
    z[y == 1] = -1/vol1
    
    return z
```


```python
D = np.diag(A.sum(axis = 0))
z = transform(A,y)

left_eq = normcut(A,y)
right_eq = 2 * ((z.T @ (D - A) @ z) / (z@D@z))
              
np.isclose(left_eq,right_eq)
```




    True




```python
np.isclose((z.T@D@np.ones(n)),0)
```




    True



## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. In the code below, I define an `orth_obj` function which handles this for you. 

We'll use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$.


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize 

z_random = transform(A,y_rand)

z_min = minimize(orth_obj,z_random)['x']
```

**Note**: there's a cheat going on here! We originally specified that the entries of $$\mathbf{z}$$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. 

## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

Does it look like we came close to correctly clustering the data? 


```python
plt.scatter(X[:,0], X[:,1], c = np.where(z_min <0,0,1))
```





    
![HW2_partE.png](/images/HW2_partE.png)
    


## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

We'll construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. Next, we'll find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. Then, we'll plot the data again, using the sign of `z_eig` as the color.


```python
L = np.linalg.inv(D)@(D - A)
Lam, U = np.linalg.eig(L)

ix = Lam.argsort()

Lam, U = Lam[ix], U[:,ix]
z_eig = U[:,1]
z_eig
```




    array([0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.09992388, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.09992388, 0.00390117, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.09992388, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.09992388, 0.00390117, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.09992388,
           0.09992388, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117])




```python
plt.scatter(X[:,0], X[:,1], c = z_eig)
```



![HW2_partF.png](/images/HW2_partF.png)
    
WOW! We successfully clustered the data! 



## Part G

Let's make a function that sythesizes all that we've acheived above.

```python
def spectral_clustering(X, epsilon):
    """
    Returns an array containing the eigenvector of the second smallest eigenvalue
    of the Laplacian matrix with respect to A, computed using the input array X and
    epsilon value
    
    X       : (Array Like) Matrix containg Euclidean coordinates of the data points
    Epsilon : (Float) Parameter to construct similarity matrix A
    """
    
    # Construct similarity matrix
    PD = euclidean_distances(X,X)
    A = np.ndarray(shape = (X.shape[0],X.shape[0]))

    A[PD < epsilon**2] = 1
    A[PD >= epsilon**2] = 0
    np.fill_diagonal(A,0)
    
    # Construct the Laplacian matrix
    D = np.diag(A.sum(axis = 0))
    L = np.linalg.inv(D)@(D - A)
    
    # Compute the eigenvector with the second-smallest eigenvalue
    # of the Laplacian matrix
    Lam, U = np.linalg.eig(L)

    ix = Lam.argsort()

    Lam, U = Lam[ix], U[:,ix]
    z_eig = U[:,1]
    
    # Return labels based on the eigenvector
    return z_eig
```


```python
spectral_clustering(X,0.4)
```




    array([0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.09992388, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.09992388, 0.00390117, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.09992388, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.09992388, 0.00390117, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.09992388,
           0.09992388, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117])



## Part H

Now let's run a few experiments using our new function, by generating different data sets using `make_moons`. 


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon))
```


    

![HW2_partH_1.png](/images/HW2_partH_1.png)
    



```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon))
```



    
![HW2_partH_2.png](/images/HW2_partH_2.png)
    



```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.005, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon))
```

 
![HW2_partH_3.png](/images/HW2_partH_3.png)
    



```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.3))
```

    
![HW2_partH_4.png](/images/HW2_partH_4.png)
    


## Part I

Now let's try our spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

    
![HW2_partI_1.png](/images/HW2_partI_1.png)
    


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
   
![HW2_partI_2.png](/images/HW2_partI_2.png)
    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.5) )
```

    
![HW2_partI_3.png](/images/HW2_partI_3.png)


YAAY! Our function can successfully separate the two circles!
    
