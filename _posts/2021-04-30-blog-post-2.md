---
layout: post
title: Assignment - Blog Post 2
---

## Introduction

In this blog post we'll be studying spectral clustering which is a way of grouping data that is represented in spectral form. Though we'll be dealing with more complex math concepts like Linear Algebra, no prior mathmematical knowledge is necessary to follow along with the code.

### Notation

In all the math below: 

- Boldface capital letters like $$\mathbf{A}$$ refer to matrices, essentially 2d numpy arrays of numbers. 
- Boldface lowercase letters like $$\mathbf{v}$$ refer to vectors, essentially 1d arrays of numbers. 
- $$\mathbf{A}\mathbf{B}$$ refers to a matrix-matrix product (`A@B`). $$\mathbf{A}\mathbf{v}$$ refers to a matrix-vector product (`A@v`).
- For more information regarding matrices and vectros please refer to this <a href="PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">collection</a> of Youtube videos going over the basics of linear lagebra. 


First, let's look at an example of simple clustering.

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

    
![HW2_phil_1.png](/images/HW2_phil_1.png)
    


As we can see in the above figure, the data points are clearly separated into two blobs or *clusters*. 

We can use *clustering* to segment the data into its distinct categories. K-means is traditionally a good way to achive this task.


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![HW2_phil_2.png](/images/HW2_phil_2.png)


This form of clustering works best when our data has a blob shape, as we'll see K-means has a harder time when the data isn't formatted as simply.


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```


    
![HW2_phil_3.png](/images/HW2_phil_3.png)
    

Here we can see there are still two clear clusters of data, the only difference is the shape being sprial like instead of blob like. K-means won't work as well in cases like this.


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

    
![HW2_phil_1.png](/images/HW2_phil_4.png)

As expected, we weren't able to distinguish these cluster with K-means.

In the rest of this blog post, we'll develop some different methods of separating the data when it is presented in spiral form.

## Similarity Matrix

First, let's construct the *similarity matrix* $$ \mathbf{A}$$. For our perposes, think of a similarity matrix as a *n* by *n* numpy array made up of 1s and 0s. We'll construct $$ \mathbf{A}$$ based on values of X in relation to a parameter we'll call *epsilon*. 

Enty $$A[i;j]$$ will be 1 if $$X[i]$$ is winthin *epsilon* distance of $$X[j]$$, and 0 otherwise. The diagonal values of $$ \mathbf{A}$$ will all be 0. 

For now, we'll use `epsilon = 0.4`. 


```python
from sklearn.metrics.pairwise import euclidean_distances

epsilon = 0.4

PD = euclidean_distances(X,X)
A = np.ndarray(shape = (n,n))

A[PD < epsilon**2] = 1
A[PD >= epsilon**2] = 0
np.fill_diagonal(A,0)
A
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           ...,
           [0., 0., 0., ..., 0., 0., 1.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 1., 0., 0.]])

The matrix `A` now contains information about which points are close (with *epsilon* distance) to one another. The next task is to partion the data points in `X` by partitinning the rows and columns of `A`.

## Binary Norm Cut Objective
 
The *binary norm cut objective* of a matrix $$\mathbf{A}$$ is defined by the following function:

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- First, we must construct two clusters of data $$C_0$$ and $$C_1$$ conatining all the data points. A data point's cluster membership is defined according to `y`. For every index i, if y[i] = 1 then i is an element of $$C_1$$ and $$C_0$$ otherwise.
- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 
- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$). The *volume* of a cluster is a measure of its size. 


#### The Cut Term

The cut term refers to the number of non-zero entries in $$ \mathbf{A}$$ such that A[i] != A[j]. We can compute the cut term by summing all the different combinations for entries A[i,j] for i and j in each cluster.

Let's write a function called `cut(A,y)` to compute the cut term. 

```python
def cut(A,y):
    
    # Determine C0 and C1 based on values of y
    C0 = np.where(y == 0)[0]
    C1 = np.where(y == 1)[0]
    
    # Sum all the different combinations for entries
    # A[i,j] for i and j in each cluster
    cut = sum(sum(A[np.ix_(C0,C1)]))
    
    return cut
```

{::options parse_block_html="true" /}
<div class="got-help">
One of my peers helped me compute C0 and C1 very quickly using np.where and to rapidly find the different combinations with np.ix_. 
</div>
{::options parse_block_html="false" /}

Let's compute the cut objective for the true clusters `y` and then we'll compare that to the cut term of a random vector of random labels of length `n`. We'll do this to show that the cut term favors true clusters as oppsoed to random ones. 


```python
cut(A,y)
```




    0.0




```python
y_rand = np.random.randint(0,2,size=n)
cut(A,y_rand)
```




    373.0

The cut objective for the true labels is *much* smaller than the cut objective for the random labels!

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 


#### The Volume Term 

Next, we must compute the *volume* term. The volume refers to the size of a cluster 

We'll first write a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$. 

Then, we'll write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 


```python
def vols(A,y):
    
    # Determine C0 and C1 based on values of y
    C0 = np.where(y == 0)[0]
    C1 = np.where(y == 1)[0]
    
    # Compute the values of vol0 and vol1 by summing
    # the values of A indexed at each cluster
    vol0 = sum(sum(A[C0]))
    vol1 = sum(sum(A[C1]))
    
    return vol0, vol1

def normcut(A,y):
    
    vol0, vol1 = vols(A,y)
    
    return cut(A,y)*(1/vol0 + 1/vol1)
```

Now, as we did with the cut term, we'll combute both the true binary norm cut objective and compare it to that of the randomly generated numbers we set earlier.


```python
print(normcut(A,y))
print(normcut(A,y_rand))
```

    0.0
    0.9877184588483989

 As expected, the normcut for the true labels is *much* smaller than the normcut of our fake labels.


## Math Tricks for Clustering

Above we've defined functions to define both the cut and volume terms with our data and similarity matrix $$ \mathbf{A}$$.

A traditional approach to clustering would be to find a cluster vector `y` such that `normcut(A,y)` is small. This might not be the most pratical approach as it could take up a lot of comupting power and time, instead we can use the math trick defined below:

Define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

First, let's define a `transform` function that'll take in our similarity matrix $$ \mathbf{A}$$ and our vector `y` to compue the new vector $$\mathbf{z}$$ that we defined above.

```python
def transform(A,y):
    
    vol0, vol1 = vols(A,y)
    
    z = np.zeros(y.shape[0])
    
    z[y == 0] = 1/vol0
    z[y == 1] = -1/vol1
    
    return z
```

Now we can show:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$ 

Where $$\mathbf{D}$$ is a diagonal matrix with nonzero entries, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.

We'll compute both sides of of our equation that relates the matrix product to the norm cut objectives are equal to one another. For this we'll use `np.isclose()` because cumputer arithmetic is not exact.


```python
D = np.diag(A.sum(axis = 0))
z = transform(A,y)

left_eq = normcut(A,y)
right_eq = 2 * ((z.T @ (D - A) @ z) / (z@D@z))
              
np.isclose(left_eq,right_eq)
```




    True

Finally, let's check the identity  $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$ , where $$\mathbb{1}$$ is the vector of n ones. This essentially says that says that there should be as many postitive entries as negative ones in $$\mathbf{z}$$.



```python
np.isclose((z.T@D@np.ones(n)),0)
```




    True



## Orthogonal Objective

In the previous part, we saw that minimizing the normuct objective is realed to minimizing the following function:

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition of $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$.

For optimization, we can substitute $$\mathbf{z}$$ for it's orthogonal complement relative to $$\mathbf{D}\mathbf{1}$$. We have defined a function below to compute this operation.

```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Now that we have the orthogonal complement, we can use the `minimize` funciton from the `scipy.optimize` package to minimize the above function with respect to $$\mathbf{z}$$.

```python
from scipy.optimize import minimize 

z_random = transform(A,y_rand)

z_min = minimize(orth_obj,z_random)['x']
```
{::options parse_block_html="true" /}
<div class="gave-help">
I pointed out to a few of my peers that it wasn't necessary to specify constraints for the minimize function. This is just a way to keep code concise.
</div>
{::options parse_block_html="false" /} 

Since only the sign of z_min[i] actually contains information about the cluster label, we can plot the different clusters by using one color for positive values z[i] and another clor for negative values z[i].




```python
plt.scatter(X[:,0], X[:,1], c = np.where(z_min <0,0,1))
```

![HW2_partE.png](/images/HW2_partE.png)

{::options parse_block_html="true" /}
<div class="got-help">
I was insipred by one of my peers who used np.where(z_min < 0,0,1) as a mask which I found super efficient! 
</div>
{::options parse_block_html="false" /} 

Wow! Looks like our minimize function did a great job a plotting the distinct sprials, but we can do even better! 
    


## Laplacian Matrix

Though the above method is able to accurately separate the clusters, it's far too slow. Instead we can achieve the task by using eigenvalues and eigenvectors. These are more examples of linear algebra concepts so you might find it useful to consult these <a href="PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">videos</a>.

First, we need to construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is the *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. 

```python
L = np.linalg.inv(D)@(D - A)
```

Next, we need to find the eigenvector corresponding to the second-smallest eigenvalue of $$\mathbf{L}$$, and call it `z_eig`. Numpy has a nifty function called `np.linalg.eig` that'll compute an eigenvector and eigenvalue pair for a given matrix. We then just need to sort that eigenvector to retreive the second smallest eignevalue.


```python
Lam, U = np.linalg.eig(L)

ix = Lam.argsort()

Lam, U = Lam[ix], U[:,ix]
z_eig = U[:,1]
z_eig
```




    array([0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.09992388, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.09992388, 0.00390117, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.09992388, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.09992388, 0.00390117, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.09992388,
           0.09992388, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117])


Now we can plot the data again, using the sign of `z_eig` as the color.

```python
plt.scatter(X[:,0], X[:,1], c = z_eig)
```



![HW2_partF.png](/images/HW2_partF.png)
    
WOW! We successfully clustered the data! 

This method is made possible by properties of the <a href='https://en.wikipedia.org/wiki/Rayleigh%E2%80%93Ritz_method'>Rayleigh-Ritz Theorm</a>.



## Putting It All Together

Let's make a function that sythesizes all that we've acheived above.

```python
def spectral_clustering(X, epsilon):
    """
    Returns an array containing the eigenvector of the second smallest eigenvalue
    of the Laplacian matrix with respect to A, computed using the input array X and
    epsilon value
    
    X       : (Array Like) Matrix containg Euclidean coordinates of the data points
    Epsilon : (Float) Parameter to construct similarity matrix A
    """
    
    # Construct similarity matrix
    PD = euclidean_distances(X,X)
    A = np.ndarray(shape = (X.shape[0],X.shape[0]))

    A[PD < epsilon**2] = 1
    A[PD >= epsilon**2] = 0
    np.fill_diagonal(A,0)
    
    # Construct the Laplacian matrix
    D = np.diag(A.sum(axis = 0))
    L = np.linalg.inv(D)@(D - A)
    
    # Compute the eigenvector with the second-smallest eigenvalue
    # of the Laplacian matrix
    Lam, U = np.linalg.eig(L)

    ix = Lam.argsort()

    Lam, U = Lam[ix], U[:,ix]
    z_eig = U[:,1]
    
    # Return labels based on the eigenvector
    return z_eig
```


```python
spectral_clustering(X,0.4)
```




    array([0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.09992388, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.09992388, 0.00390117, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.09992388, 0.09992388, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.09992388, 0.00390117, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.09992388, 0.00390117, 0.09992388,
           0.09992388, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.09992388, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.00390117,
           0.00390117, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.00390117, 0.00390117, 0.00390117, 0.09992388,
           0.09992388, 0.00390117, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.00390117, 0.09992388, 0.09992388, 0.09992388,
           0.00390117, 0.09992388, 0.00390117, 0.09992388, 0.09992388,
           0.09992388, 0.09992388, 0.00390117, 0.00390117, 0.00390117])



## Testing on Spiral Data

Now let's run a few experiments using our new function, by generating different data sets using `make_moons`. 


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon))
```


    

![HW2_partH_1.png](/images/HW2_partH_1.png)
    



```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon))
```



    
![HW2_partH_2.png](/images/HW2_partH_2.png)
 


```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.02, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.3))
```

    
![HW2_partH_4.png](/images/HW2_partH_4.png)

```python
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.005, random_state=None)
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,epsilon))
```

 
![HW2_partH_3.png](/images/HW2_partH_3.png)


As we can see above, as the noise gets smaller, the spirals become more "refined". They appear more like a single line as opposed to a larger blob of data.
    


## Testing on Circular Data

Now let's try our spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

    
![HW2_partI_1.png](/images/HW2_partI_1.png)
    


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
   
![HW2_partI_2.png](/images/HW2_partI_2.png)
    
```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.4) )
```

![HW2_partI_7.png](/images/HW2_partI_7.png)


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.5) )
```
    
![HW2_partI_3.png](/images/HW2_partI_3.png)

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.6) )
```
![HW2_partI_6.png](/images/HW2_partI_6.png)

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.7) )
```
![HW2_partI_5.png](/images/HW2_partI_5.png)

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X,0.9))
```

![HW2_partI_4.png](/images/HW2_partI_4.png)



YAAY! Our function can successfully separate the two circles!

It seems that we're only able to properly distinguish the circles for epsilon values between 0.4 and 0.6 exclusive. Epsilon values below 0.4 generate an error message.
    
